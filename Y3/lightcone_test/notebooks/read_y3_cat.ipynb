{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESI Y3 redshift 'ERRORS' Analysis\n",
    "\n",
    "This notebook analyzes redshift 'ERRORS' (uncertainties and catastrophics) in DESI Y3 Abacus mock data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import fitsio\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table, vstack\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from cosmoprimo.fiducial import AbacusSummit\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('/global/homes/s/shengyu/project_rc/main/Y3/')\n",
    "from helper import REDSHIFT_VSMEAR, Y3_NRAN, Y3_ZRANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 299792 # speed of light in km/s\n",
    "cosmo = AbacusSummit() \n",
    "\n",
    "def get_edges(corr_type='smu', bin_type='lin'):\n",
    "    if bin_type == 'log':\n",
    "        sedges = np.geomspace(0.01, 100., 49)\n",
    "    elif bin_type == 'lin':\n",
    "        sedges = np.linspace(0., 200, 201)\n",
    "    else:\n",
    "        raise ValueError('bin_type must be one of [\"log\", \"lin\"]')\n",
    "    if corr_type == 'smu':\n",
    "        edges = (sedges, np.linspace(-1., 1., 201)) #s is input edges and mu evenly spaced between -1 and 1\n",
    "    elif corr_type == 'rppi':\n",
    "        if bin_type == 'lin':\n",
    "            edges = (sedges, np.linspace(-40., 40, 101)) #transverse and radial separations are coded to be the same here\n",
    "        else:\n",
    "            edges = (sedges, np.linspace(-40., 40., 81))\n",
    "    elif corr_type == 'theta':\n",
    "        edges = (np.linspace(0., 4., 101),)\n",
    "    else:\n",
    "        raise ValueError('corr_type must be one of [\"smu\", \"rppi\", \"theta\"]')\n",
    "    return edges\n",
    "\n",
    "def _concatenate(arrays):\n",
    "    if isinstance(arrays[0], (tuple, list)):  # e.g., list of bitwise weights for first catalog\n",
    "        array = [np.concatenate([arr[iarr] for arr in arrays], axis=0) for iarr in range(len(arrays[0]))]\n",
    "    else:\n",
    "        array = np.concatenate(arrays, axis=0)  # e.g. individual weights for first catalog\n",
    "    return array\n",
    "\n",
    "def get_positions_weights(catalog, tracer, region, zrange, weight_type = 'default', systematics_type = None):\n",
    "    z2d = cosmo.comoving_radial_distance\n",
    "    toret = []\n",
    "    if isinstance(catalog, (tuple, list)):  # list of catalogs, one for each region\n",
    "        for cat in catalog:\n",
    "            toret += get_positions_weights(cat, tracer, region, zrange, weight_type = 'default', systematics_type = None)\n",
    "    else:\n",
    "        if systematics_type == 'dv-obs':\n",
    "            import time\n",
    "            from Y3_redshift_systematics import vsmear, vsmear_modelling\n",
    "            random_seed = int(time.time() * 1000) % (2**32)\n",
    "            zmid = (zrange[0]+zrange[1])/2\n",
    "            dv = vsmear(tracer, zmin=zrange[0], zmax=zrange[1], Ngal = len(catalog), seed=random_seed)\n",
    "            dz = dv*(1+zmid)/c # ∆z = ∆v*(1+z)/c\n",
    "            catalog['Z'] += dz\n",
    "        maskz = (catalog['Z'] >= zrange[0]) & (catalog['Z'] < zrange[1]) \n",
    "        mask  = maskz & select_region(catalog['RA'], catalog['DEC'], region=region)\n",
    "        cat = catalog[mask]\n",
    "        dist = z2d(cat['Z'])\n",
    "        positions = [cat['RA'].data, cat['DEC'].data, dist]\n",
    "        weights = np.ones(len(cat), dtype=bool)\n",
    "        if 'default' in weight_type:\n",
    "            weights = cat['WEIGHT'].data\n",
    "        toret.append((np.array(positions), np.array(weights)))\n",
    "        # toret.append((positions, weights))\n",
    "    return toret\n",
    "\n",
    "def normalize_data_randoms_weights(data_weights, randoms_weights, weight_attrs=None):\n",
    "    # Renormalize randoms / data for each input catalogs\n",
    "    # data_weights should be a list (for each N/S catalogs) of weights\n",
    "    import inspect\n",
    "    from pycorr.twopoint_counter import _format_weights, get_inverse_probability_weight\n",
    "    if weight_attrs is None: weight_attrs = {}\n",
    "    weight_attrs = {k: v for k, v in weight_attrs.items() if k in inspect.getargspec(get_inverse_probability_weight).args}\n",
    "    wsums, weights = {}, {}\n",
    "    for name, catalog_weights in zip(['data', 'randoms'], [data_weights, randoms_weights]):\n",
    "        wsums[name], weights[name] = [], []\n",
    "        for w in catalog_weights:\n",
    "            w, nbits = _format_weights(w, copy=True)  # this will sort bitwise weights first, then single individual weight\n",
    "            iip = get_inverse_probability_weight(w[:nbits], **weight_attrs) if nbits else 1.\n",
    "            iip = iip * w[nbits]\n",
    "            wsums[name].append(iip.sum())\n",
    "            weights[name].append(w)\n",
    "    wsum_data, wsum_randoms = sum(wsums['data']), sum(wsums['randoms'])\n",
    "    for icat, w in enumerate(weights['randoms']):\n",
    "        factor = wsums['data'][icat] / wsums['randoms'][icat] * wsum_randoms / wsum_data\n",
    "        w[-1] *= factor\n",
    "        print('Rescaling randoms weights of catalog {:d} by {:.4f}.'.format(icat, factor), flush=True)\n",
    "    return weights['data'], weights['randoms']\n",
    "\n",
    "def concatenate_data_randoms(data, randoms=None, **kwargs):\n",
    "    if randoms is None:\n",
    "        positions, weights = data\n",
    "        return _concatenate(positions), _concatenate(weights)\n",
    "    positions, weights = {}, {}\n",
    "    for name in ['data', 'randoms']:\n",
    "        positions[name], weights[name] = locals()[name]\n",
    "    for name in positions:\n",
    "        concatenated = not isinstance(positions[name][0][0], (tuple, list))  # first catalog, unconcatenated [RA, DEC, distance] (False) or concatenated RA (True)?\n",
    "        if concatenated:\n",
    "            positions[name] = _concatenate(positions[name])\n",
    "        else: \n",
    "            positions[name] = [_concatenate([p[i] for p in positions[name]]) for i in range(len(positions['randoms'][0]))]\n",
    "    data_weights, randoms_weights = [], []\n",
    "    if concatenated:\n",
    "        wd, wr = normalize_data_randoms_weights(weights['data'], weights['randoms'], weight_attrs=kwargs.get('weight_attrs', None))\n",
    "        weights['data'], weights['randoms'] = _concatenate(wd), _concatenate(wr)\n",
    "    else:\n",
    "        for i in range(len(weights['randoms'][0])):\n",
    "            wd, wr = normalize_data_randoms_weights(weights['data'], [w[i] for w in weights['randoms']], weight_attrs=kwargs.get('weight_attrs', None))\n",
    "            data_weights.append(_concatenate(wd))\n",
    "            randoms_weights.append(_concatenate(wr))\n",
    "        weights['data'] = data_weights[0]\n",
    "        for wd in data_weights[1:]:\n",
    "            for w0, w in zip(weights['data'], wd): assert np.all(w == w0)\n",
    "        weights['randoms'] = randoms_weights\n",
    "    return [(positions[name], weights[name]) for name in ['data', 'randoms']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pscratch/sd/s/shengyu/galaxies/catalogs/DA2/Abacus_v4_1/altmtl0/kibo-v1/mock0/LSScats/ELG_LOPnotqso_NGC_clustering.dat.fits\n",
      "Y3 Abacus data catalog: 4279844\n",
      "/pscratch/sd/s/shengyu/galaxies/catalogs/DA2/Abacus_v4_1/altmtl0/kibo-v1/mock0/LSScats/ELG_LOPnotqso_NGC_0_clustering.ran.fits\n",
      "Y3 Abacus random catalog: 17296184\n",
      "/pscratch/sd/s/shengyu/galaxies/catalogs/Y1/Abacus_v4_2/altmtl0/iron/mock0/LSScats/ELG_LOPnotqso_NGC_clustering.dat.fits\n",
      "Y1 Abacus data catalog: 1813296\n",
      "/pscratch/sd/s/shengyu/galaxies/catalogs/Y1/Abacus_v4_2/altmtl0/iron/mock0/LSScats/ELG_LOPnotqso_NGC_0_clustering.ran.fits\n",
      "Y1 Abacus random catalog: 9825715\n"
     ]
    }
   ],
   "source": [
    "nran_list = {'ELG_LOPnotqso':10, 'LRG':8, 'QSO':4}\n",
    "\n",
    "survey  ='DA2'\n",
    "mockver ='Abacus_v4_1'\n",
    "specver ='kibo-v1'\n",
    "tracer  ='ELG_LOPnotqso' # LRG, ELG_LOPnotqso, QSO\n",
    "nran = nran_list[tracer]\n",
    "zrange = Y3_ZRANGE[tracer][0]\n",
    "MOCKNUM = 0\n",
    "\n",
    "file_path = f'/pscratch/sd/s/shengyu/galaxies/catalogs/{survey}/{mockver}/altmtl{MOCKNUM}/{specver}/mock{MOCKNUM}/LSScats'\n",
    "catalog_fn = file_path+f'/{tracer}_clustering.dat.fits'\n",
    "# catalog_fn = f'/global/cfs/cdirs/desi/survey/catalogs/DA2/mocks/SecondGenMocks/AbacusSummit_v4_1/altmtl0/kibo-v1/mock0/LSScats/QSO_clustering.dat.fits'\n",
    "# print(catalog_fn)\n",
    "# catalog=Table(fitsio.read(catalog_fn, columns=['RA', 'DEC', 'Z', 'WEIGHT']))\n",
    "# print(catalog.colnames, '\\n', len(catalog))\n",
    "\n",
    "catalog_fn = file_path+f'/{tracer}_NGC_clustering.dat.fits'\n",
    "print(catalog_fn)\n",
    "catalog=Table(fitsio.read(catalog_fn, columns=['RA', 'DEC', 'Z', 'WEIGHT']))\n",
    "print('Y3 Abacus data catalog:',len(catalog))\n",
    "\n",
    "catalog_fn = file_path+f'/{tracer}_NGC_0_clustering.ran.fits'\n",
    "print(catalog_fn)\n",
    "catalog=Table(fitsio.read(catalog_fn, columns=['RA', 'DEC', 'Z', 'WEIGHT']))\n",
    "print('Y3 Abacus random catalog:',len(catalog))\n",
    "\n",
    "# catalog_fn = file_path+f'/{tracer}_SGC_clustering.dat.fits'\n",
    "# print(catalog_fn)\n",
    "# catalog=Table(fitsio.read(catalog_fn, columns=['RA', 'DEC', 'Z', 'WEIGHT']))\n",
    "# print(catalog.colnames, '\\n', len(catalog))\n",
    "\n",
    "catalog_fn = f'/pscratch/sd/s/shengyu/galaxies/catalogs/Y1/Abacus_v4_2/altmtl0/iron/mock0/LSScats/{tracer}_NGC_clustering.dat.fits'\n",
    "print(catalog_fn)\n",
    "catalog=Table(fitsio.read(catalog_fn, columns=['RA', 'DEC', 'Z', 'WEIGHT']))\n",
    "print('Y1 Abacus data catalog:',len(catalog))\n",
    "\n",
    "catalog_fn = f'/pscratch/sd/s/shengyu/galaxies/catalogs/Y1/Abacus_v4_2/altmtl0/iron/mock0/LSScats/{tracer}_NGC_0_clustering.ran.fits'\n",
    "print(catalog_fn)\n",
    "catalog=Table(fitsio.read(catalog_fn, columns=['RA', 'DEC', 'Z', 'WEIGHT']))\n",
    "print('Y1 Abacus random catalog:',len(catalog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer = 'LRG'\n",
    "region = 'NGC'\n",
    "nsplits = 10\n",
    "basedir = f'/pscratch/sd/s/shengyu/galaxies/catalogs/DA2/Abacus_v4_1/altmtl0/kibo-v1/mock0/LSScats'\n",
    "ran_mock_fns = [basedir+f'/{tracer}_{region}_{{}}_clustering.ran.fits'.format(i) for i in range(2)]\n",
    "ran_mocks = vstack([Table(fitsio.read(v, columns=['RA', 'DEC', 'Z', 'WEIGHT'])) for v in ran_mock_fns])\n",
    "randoms = get_positions_weights(np.array_split(ran_mocks, nsplits), tracer, region, zrange, weight_type = 'default', systematics_type = None)\n",
    "\n",
    "data_mock_fn = basedir+f'/{tracer}_NGC_clustering.dat.fits'\n",
    "data_mocks=Table(fitsio.read(catalog_fn, columns=['RA', 'DEC', 'Z', 'WEIGHT']))\n",
    "data = get_positions_weights(data_mocks, tracer, region, zrange, weight_type = 'default', systematics_type = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "randoms_positions = [entry[0] for entry in randoms]\n",
    "randoms_weights = [entry[1] for entry in randoms]\n",
    "randoms = (randoms_positions, randoms_weights)\n",
    "\n",
    "data_positions, data_weights = data\n",
    "data = (data_positions, data_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(corr_type='smu', bin_type='lin'):\n",
    "    if bin_type == 'log':\n",
    "        sedges = np.geomspace(0.01, 100., 49)\n",
    "    elif bin_type == 'lin':\n",
    "        sedges = np.linspace(0., 200, 201)\n",
    "    else:\n",
    "        raise ValueError('bin_type must be one of [\"log\", \"lin\"]')\n",
    "    if corr_type == 'smu':\n",
    "        edges = (sedges, np.linspace(-1., 1., 201)) #s is input edges and mu evenly spaced between -1 and 1\n",
    "    elif corr_type == 'rppi':\n",
    "        if bin_type == 'lin':\n",
    "            edges = (sedges, np.linspace(-40., 40, 101)) #transverse and radial separations are coded to be the same here\n",
    "        else:\n",
    "            edges = (sedges, np.linspace(-40., 40., 81))\n",
    "    elif corr_type == 'theta':\n",
    "        edges = (np.linspace(0., 4., 101),)\n",
    "    else:\n",
    "        raise ValueError('corr_type must be one of [\"smu\", \"rppi\", \"theta\"]')\n",
    "    return edges\n",
    "\n",
    "def get_positions_weights(catalog, tracer, region, zrange, weight_type = 'default', systematics_type = None):\n",
    "    z2d = cosmo.comoving_radial_distance\n",
    "    toret = []\n",
    "    if isinstance(catalog, (tuple, list)):  # list of catalogs, one for each region\n",
    "        for cat in catalog:\n",
    "            toret += get_positions_weights(cat, tracer, region, zrange, weight_type = 'default', systematics_type = None)\n",
    "    else:\n",
    "        if systematics_type == 'dv-obs':\n",
    "            import time\n",
    "            from Y3_redshift_systematics import vsmear, vsmear_modelling\n",
    "            random_seed = int(time.time() * 1000) % (2**32)\n",
    "            zmid = (zrange[0]+zrange[1])/2\n",
    "            dv = vsmear(tracer, zmin=zrange[0], zmax=zrange[1], Ngal = len(catalog), seed=random_seed)\n",
    "            dz = dv*(1+zmid)/c # ∆z = ∆v*(1+z)/c\n",
    "            catalog['Z'] += dz\n",
    "        maskz = (catalog['Z'] >= zrange[0]) & (catalog['Z'] < zrange[1]) \n",
    "        mask  = maskz & select_region(catalog['RA'], catalog['DEC'], region=region)\n",
    "        cat = catalog[mask]\n",
    "        dist = z2d(cat['Z'])\n",
    "        positions = [cat['RA'].data, cat['DEC'].data, dist]\n",
    "        weights = np.ones(len(cat), dtype=bool)\n",
    "        if 'default' in weight_type:\n",
    "            weights = cat['WEIGHT'].data\n",
    "        # toret.append((np.array(positions), np.array(weights)))\n",
    "        toret.append((np.array(positions), np.array(weights)))\n",
    "    return toret\n",
    "\n",
    "def normalize_data_randoms_weights(data_weights, randoms_weights, weight_attrs=None):\n",
    "    if mpicomm is None:\n",
    "        mpicomm = mpi.COMM_WORLD\n",
    "    # Renormalize randoms / data for each input catalogs\n",
    "    # data_weights should be a list (for each N/S catalogs) of weights\n",
    "    import inspect\n",
    "    from pycorr.twopoint_counter import _format_weights, get_inverse_probability_weight\n",
    "    if weight_attrs is None: weight_attrs = {}\n",
    "    weight_attrs = {k: v for k, v in weight_attrs.items() if k in inspect.getargspec(get_inverse_probability_weight).args}\n",
    "    def sum_weights(*weights):\n",
    "        sum_weights, formatted_weights = [], []\n",
    "        for weight in weights:\n",
    "            weight, nbits = _format_weights(weight, copy=True)  # this will sort bitwise weights first, then single individual weight\n",
    "            iip = (get_inverse_probability_weight(weight[:nbits], **weight_attrs) if nbits else 1.) * weight[nbits]\n",
    "            sum_weights.append(mpicomm.allreduce(np.sum(iip)))\n",
    "            formatted_weights.append(weight)\n",
    "        return sum_weights, formatted_weights\n",
    "    data_sum_weights, data_weights = sum_weights(*data_weights)\n",
    "    randoms_sum_weights, randoms_weights = sum_weights(*randoms_weights)\n",
    "    all_data_sum_weights, all_randoms_sum_weights = sum(data_sum_weights), sum(randoms_sum_weights)\n",
    "    for icat, rw in enumerate(randoms_weights):\n",
    "        if randoms_sum_weights[icat] != 0:\n",
    "            factor = data_sum_weights[icat] / randoms_sum_weights[icat] * all_randoms_sum_weights / all_data_sum_weights\n",
    "            rw[-1] *= factor\n",
    "    return data_weights, randoms_weights\n",
    "\n",
    "def concatenate_data_randoms(data_positions_weights, *randoms_positions_weights, weight_attrs=None, randoms_splits_size=None, concatenate=None, mpicomm=None):\n",
    "    # data_positions_weights: list of (positions, weights) (for each region)\n",
    "    # randoms_positions_weights: list of (positions, weights) (for each region)\n",
    "    def _concatenate(array):\n",
    "        if isinstance(array[0], (tuple, list)):\n",
    "            array = [np.concatenate([arr[iarr] for arr in array], axis=0) if array[0][iarr] is not None else None for iarr in range(len(array[0]))]\n",
    "        elif array is not None:\n",
    "            array = np.concatenate(array)  # e.g. Z column\n",
    "        return array\n",
    "    data_positions, data_weights = tuple(_concatenate([pw[i] for pw in data_positions_weights]) for i in range(len(data_positions_weights[0]) - 1)), [pw[-1] for pw in data_positions_weights]\n",
    "    if not randoms_positions_weights:\n",
    "        data_weights = _concatenate(data_weights)\n",
    "        return data_positions + (data_weights,)\n",
    "    print('data proceed finish here',flush=True)\n",
    "    list_randoms_positions = tuple([_concatenate([pw[i] for pw in rr]) for rr in randoms_positions_weights] for i in range(len(randoms_positions_weights[0][0]) - 1))\n",
    "    list_randoms_weights = [[pw[-1] for pw in rr] for rr in randoms_positions_weights]\n",
    "    print('random proceed finish here',flush=True)\n",
    "    import pdb; pdb.set_trace()\n",
    "    for iran, randoms_weights in enumerate(list_randoms_weights):\n",
    "        list_randoms_weights[iran] = _concatenate(normalize_data_randoms_weights(data_weights, randoms_weights, weight_attrs=weight_attrs)[1])\n",
    "    data_weights = _concatenate(data_weights)\n",
    "    list_randoms_positions_weights = list_randoms_positions + (list_randoms_weights,)\n",
    "    if concatenate:\n",
    "        list_randoms_positions_weights = tuple(_concatenate(rr) for rr in list_randoms_positions_weights)\n",
    "    return data_positions + (data_weights,), list_randoms_positions_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 299792 # speed of light in km/s\n",
    "cosmo = AbacusSummit() \n",
    "tracer = 'LRG'\n",
    "region = 'NGC'\n",
    "nsplits = 10\n",
    "zrange = Y3_ZRANGE[tracer][0]\n",
    "basedir = f'/pscratch/sd/s/shengyu/galaxies/catalogs/DA2/Abacus_v4_1/altmtl0/kibo-v1/mock0/LSScats'\n",
    "ran_mock_fns = [basedir+f'/{tracer}_{region}_{{}}_clustering.ran.fits'.format(i) for i in range(2)]\n",
    "ran_mocks = vstack([Table(fitsio.read(v, columns=['RA', 'DEC', 'Z', 'WEIGHT'])) for v in ran_mock_fns])\n",
    "randoms = get_positions_weights(np.array_split(ran_mocks, nsplits), tracer, region, zrange, weight_type = 'default', systematics_type = None)\n",
    "\n",
    "data_mock_fn = basedir+f'/{tracer}_NGC_clustering.dat.fits'\n",
    "data_mocks=Table(fitsio.read(data_mock_fn, columns=['RA', 'DEC', 'Z', 'WEIGHT']))\n",
    "data = get_positions_weights(data_mocks, tracer, region, zrange, weight_type = 'default', systematics_type = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pycorr import mpi\n",
    "c = 299792 # speed of light in km/s\n",
    "cosmo = AbacusSummit() \n",
    "def _concatenate(array):\n",
    "    if isinstance(array[0], (tuple, list)):\n",
    "        array = [np.concatenate([arr[iarr] for arr in array], axis=0) if array[0][iarr] is not None else None for iarr in range(len(array[0]))]\n",
    "    elif array is not None:\n",
    "        array = np.concatenate(array)  # e.g. Z column\n",
    "    return array\n",
    "data_positions, data_weights = tuple(_concatenate([pw[i] for pw in data]) for i in range(len(data[0]) - 1)), [pw[-1] for pw in data]\n",
    "print('data proceed finish here',flush=True)\n",
    "# list_randoms_positions = tuple([_concatenate([pw[i] for pw in rr]) for rr in randoms] for i in range(len(randoms[0][0]) - 1))\n",
    "# list_randoms_weights = [[pw[-1] for pw in rr] for rr in randoms]\n",
    "# print('random proceed finish here',flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hod_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
